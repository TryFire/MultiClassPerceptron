{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "and tack a look on the datas and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Les', 'commotions', 'cérébrales', 'sont', 'devenu', 'si', 'courantes', 'dans', 'ce', 'sport', \"qu'\", 'on', 'les', 'considére', 'presque', 'comme', 'la', 'routine', '.']\n",
      "['DET', 'NOUN', 'ADJ', 'VERB', 'VERB', 'ADV', 'ADJ', 'ADP', 'DET', 'NOUN', 'SCONJ', 'PRON', 'PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['HG3', ';)', 'si', 'ous', 'voulez', 'x))))']\n",
      "['PROPN', 'INTJ', 'SCONJ', 'PRON', 'VERB', 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "train_set = json.load(open('fr.ud.train.json'))\n",
    "dev_set = json.load(open('fr.ud.dev.json'))\n",
    "test_set = json.load(open('fr.ud.test.json'))\n",
    "minecraft_set = json.load(open('minecraft.json'))\n",
    "for words, labels in train_set:\n",
    "    print(words)\n",
    "    print(labels)\n",
    "    break\n",
    "for words, labels in minecraft_set:\n",
    "    print(words)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all possible labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON',\n",
       " 'NOUN',\n",
       " 'SYM',\n",
       " 'NUM',\n",
       " 'X',\n",
       " 'ADJ',\n",
       " 'DET',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'ADV',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'ADP+DET',\n",
       " 'CCONJ',\n",
       " 'ADP+PRON',\n",
       " 'INTJ',\n",
       " 'SCONJ',\n",
       " 'PART']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def all_lables(data_set):\n",
    "    res = set()\n",
    "    for words, labels in data_set:\n",
    "        res = res | set(labels)\n",
    "    return list(res)\n",
    "labels = all_lables(train_set)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Each example (word) of the dataset is going to be represented by a feature vector. Now we are considering the following *feature patterns*\n",
    " \n",
    "- the 3 last characters of the word\n",
    "- the last character of the word\n",
    "- the first character of the word\n",
    "- the word\n",
    "- a binary feature indicating whether the word starts with a capital letter or not\n",
    "- a binary feature indicating whether the word is made only of capital letters or not\n",
    "- the word at position i − 1 and i − 2\n",
    "- the word at position i + 1 and i + 2\n",
    "\n",
    "where i is the index of the word in its sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse(i, context):\n",
    "    word = context[i]\n",
    "    res = []\n",
    "    res.append(('3_last',word[-3:]))\n",
    "    res.append(('last_c',word[-1]))\n",
    "    res.append(('firt_c',word[0]))\n",
    "    res.append(('word',word))\n",
    "    res.append(('start_capital',1 if word.istitle() else 0))\n",
    "    res.append(('only_capital',1 if word.isupper() else 0))\n",
    "    res.append(('i-1',context[i-1] if i>0 else None))\n",
    "    res.append(('i-2',context[i-2] if i>1 else None))\n",
    "    res.append(('i+1',context[i+1] if len(context)>i+1 else None))\n",
    "    res.append(('i+2',context[i+2] if len(context)>i+2 else None))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        # Each feature gets its own weight vector, with one weight for\n",
    "        # each possible label\n",
    "        self.weights = defaultdict(lambda: defaultdict(float))\n",
    "        # The accumulated values of the weight vector at the t-th\n",
    "        # iteration: sum_{i=1}^{n - 1} w_i\n",
    "        #\n",
    "        # The current value (w_t) is not yet added. The key of this\n",
    "        # dictionary is a pair (feature, label)\n",
    "        self._accum = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging.\n",
    "        self._last_update = defaultdict(int)\n",
    "        # Number of examples seen\n",
    "        self.n_updates = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return\n",
    "        the best class.'''\n",
    "        labels, labels_score = self.score(features)\n",
    "        return labels[max(enumerate(labels_score),key=lambda x: x[1])[0]]\n",
    "    \n",
    "    def predict_all(self, features):\n",
    "        predicts = []\n",
    "        for f in features:\n",
    "            predicts.append(self.predict(f))\n",
    "        return predicts\n",
    "    \n",
    "    def fit(self, train_set, train_labels):\n",
    "        for features, true_label in zip(train_set, train_labels):\n",
    "            self.update(true_label, self.predict(features), features) \n",
    "    \n",
    "    def score(self, features, labels=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        - features, an iterable\n",
    "             a sequence of binary features. Each feature must be\n",
    "             hashable. WARNING: the `value' of the feature is always\n",
    "             assumed to be 1.\n",
    "        - labels, a subset of self.labels\n",
    "             if not None, the score is computed only for these labels\n",
    "        \"\"\" \n",
    "        if not labels:\n",
    "            labels_score = [0 for i in range(len(self.labels))]\n",
    "            for f in features:\n",
    "                for label in self.labels:\n",
    "                    labels_score[self.labels.index(label)] += self.weights[f][label]\n",
    "            return self.labels, labels_score\n",
    "        else :\n",
    "            labels_score = [0 for i in range(len(labels))]\n",
    "            for f in features:\n",
    "                for label in labels:\n",
    "                    labels_score[labels.index(label)] += self.weights[f][label]\n",
    "            return labels, labels_score\n",
    "        \n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        def upd_feat(label, feature, v):\n",
    "            param = (label, feature)\n",
    "            self._accum[param] += (self.n_updates -\n",
    "                                   self._last_update[param]) * self.weights[feature][label]\n",
    "            self._last_update[param] = self.n_updates\n",
    "            self.weights[feature][label] += v\n",
    "            \n",
    "        self.n_updates += 1\n",
    "\n",
    "        if truth == guess:\n",
    "            return\n",
    "\n",
    "        for f in features:\n",
    "            upd_feat(truth, f, 1.0)\n",
    "            upd_feat(guess, f, -1.0)\n",
    "\n",
    "    def average_weights(self):\n",
    "        \"\"\"\n",
    "        Average weights of the perceptron\n",
    "\n",
    "        Training can no longer be resumed.\n",
    "        \"\"\"\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for label, w in weights.items():\n",
    "                param = (label, feat)\n",
    "                # Be careful not to add 1 to take into account the\n",
    "                # last weight vector (without increasing the number of\n",
    "                # iterations in the averaging)\n",
    "                total = self._accum[param] + \\\n",
    "                    (self.n_updates + 1 - self._last_update[param]) * w\n",
    "                averaged = round(total / self.n_updates, 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[label] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"\n",
    "        Serialization of a perceptron\n",
    "\n",
    "        We are only serializing the weight vector as a dictionnary\n",
    "        because defaultdict with lambda can not be serialized.\n",
    "        \"\"\"\n",
    "        # should we also serialize the other attributes to allow\n",
    "        # learning to continue?\n",
    "        return {\"weights\": {k: v for k, v in self.weights.items()}}\n",
    "\n",
    "    def __setstate__(self, data):\n",
    "        \"\"\"\n",
    "        De-serialization of a perceptron\n",
    "        \"\"\"\n",
    "\n",
    "        self.weights = defaultdict(lambda: defaultdict(float), data[\"weights\"])\n",
    "        # ensure we are no longer able to continue training\n",
    "        self._accum = None\n",
    "        self._last_update = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model with all the labels possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words, true_labels in train_set:\n",
    "    for i in range(len(words)):\n",
    "        features = sparse(i, words)\n",
    "        label_predict = perceptron.predict(features)\n",
    "        perceptron.update(true_labels[i], label_predict, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_test = []\n",
    "all_true_labels = []\n",
    "for words, true_labels in test_set:\n",
    "    for i in range(len(words)):\n",
    "        features = sparse(i, words)\n",
    "        label_predict = perceptron.predict(features)\n",
    "        predicts_test.append(label_predict)\n",
    "        all_true_labels.append(true_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function of *accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def accuracy(predicts_test, all_true_labels):\n",
    "    return np.sum(np.array(predicts_test) == np.array(all_true_labels)) / len(predicts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163416136316977"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predicts_test, all_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the matrix of dataset and the target from the datas origin and then train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data):\n",
    "    dataset = []\n",
    "    labelset = []\n",
    "    for words, labels in data:\n",
    "        for i in range(len(words)):\n",
    "            dataset.append(sparse(i, words))\n",
    "            labelset.append(labels[i])\n",
    "    return dataset, labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels = build_dataset(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(labels)\n",
    "p.fit(train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163416136316977"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset, test_labels = build_dataset(test_set)\n",
    "accuracy(p.predict_all(test_dataset), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
