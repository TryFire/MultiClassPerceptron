{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = json.load(open('corpus/fr/fr.gsd.train.json'))\n",
    "dev_set = json.load(open('corpus/fr/fr.gsd.dev.json'))\n",
    "test_set = json.load(open('corpus/fr/fr.gsd.test.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a vue on data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Les', 'commotions', 'cérébrales', 'sont', 'devenu', 'si', 'courantes', 'dans', 'ce', 'sport', \"qu'\", 'on', 'les', 'considére', 'presque', 'comme', 'la', 'routine', '.']\n",
      "['DET', 'NOUN', 'ADJ', 'AUX', 'VERB', 'ADV', 'ADJ', 'ADP', 'DET', 'NOUN', 'SCONJ', 'PRON', 'PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "for k,v in train_set[:20]:\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing shortly the different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbr of sentences in train_set : 14450\n",
      "nbr of sentences in dev_set : 1476\n",
      "nbr of sentences in test_set : 416\n"
     ]
    }
   ],
   "source": [
    "# the data may be come from the newspaper\n",
    "\n",
    "# the number of sentences in each data_set\n",
    "print('nbr of sentences in train_set : %d'% len(train_set))\n",
    "print('nbr of sentences in dev_set : %d'% len(dev_set))\n",
    "print('nbr of sentences in test_set : %d'% len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three measures of the noisiness of a corpus\n",
    "\n",
    "- The percentage of **Out-of-Vocabulary (OOV) words**, i.e. words appearing in the test set that are not contained on the train set;\n",
    "- **[The KL divergence of 3-grams characters](https://aclweb.org/anthology/W16-3905)** distributions estimated on the train and test sets\n",
    "- perplexity on the test set of a (word level) Language Model estimated on the test\n",
    "set. The language model can be estimated by KenLM (this tools can also be used\n",
    "to compute the perpexlity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(data_set):\n",
    "    '''\n",
    "    data_set : input data in dimension of (N,M)\n",
    "    \n",
    "    return : the set of words appearing the data_set\n",
    "    '''\n",
    "    words = set()\n",
    "    for k,v in data_set:\n",
    "        words = words.union(k)\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The percentage of Out-of-Vocabulary (OOV) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train_set = extract_words(train_set)\n",
    "words_test_set = extract_words(test_set)\n",
    "oov = words_test_set.difference(words_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('percentage of oov in train_set : %.2f%%'%(len(oov)/len(words_train_set)*100))\n",
    "print('percentage of oov in test_set : %.2f%%'%(len(oov)/len(words_test_set)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:// The KL divergence of 3-grams characters distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we try to mesure the noisiness of a corpus, because the noisiness can do large impact on the performance of model. And a good knowledge can help us to build and train a better model.\n",
    "- here the metric gives a low value means that there are few noisiness in the corpus, else much noisiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:// compute the value of the different metric for the different combination of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### Considering the features\n",
    "\n",
    "- **the word**  *explain : this feature can directly get the labels which are related to the word*\n",
    "#### *Windows*\n",
    "- **a window of 5 words around the word** of interest (i.e. the word we want to predict a label for, the two previous words and the two following words) *explain: these features can make the label more accurate*\n",
    "#### *Word features*\n",
    "In this section, we consider these sources of information equally important and normalize each of the four component vectors to unit length\n",
    "- **counts of left neighbors**\n",
    "- **counts of right neighbors**\n",
    "- **binary suffix features**\n",
    "- **binary shape features**\n",
    "#### *Distributional features*\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_window(i, context, l=2):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    l : a window of size is 2*l+1\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    # the result of features\n",
    "    res = []\n",
    "    # the word\n",
    "    word = context[i]\n",
    "    # add the word to the list\n",
    "    res.append(word)\n",
    "    punct = [',','.','(',')',':',';','/','?','«','\"', '»']\n",
    "    if word in punct:\n",
    "        return res\n",
    "    for k in range(1,l+1):\n",
    "        # the word of index(word) - k\n",
    "        res.append('win_i-%d'%k + context[i-k] if i-k>=0 else 'none')\n",
    "        # the word of index(word) + k\n",
    "        res.append('win_i+%d'%k+context[i+k] if i+k<len(context) else 'none')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_suffix(i, context, s=3):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    s : the 1,2,...,s suffix lettre of the word\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    \n",
    "    # the result of features\n",
    "    res = []\n",
    "    # the word\n",
    "    word = context[i]\n",
    "    for k in range(-1, -(s+1), -1):\n",
    "        # the feature of k-th suffix of the word\n",
    "        res.append('%d-th_suffix_'%k + word[k:])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_shape(i, context):\n",
    "    '''\n",
    "    i : the index of the word in the context\n",
    "    context : the sentence\n",
    "    \n",
    "    return : list of features which are tuple (feature_name, value)\n",
    "    '''\n",
    "    def has_digit(s):\n",
    "        '''\n",
    "        check if a string has digit or nor\n",
    "        '''\n",
    "        return any(c.isdigit() for c in s)\n",
    "    \n",
    "    # the result of features\n",
    "    res = []\n",
    "    # the word\n",
    "    word = context[i]\n",
    "    \n",
    "    punct = [',','.','(',')',':',';','/','?','«','\"', '»']\n",
    "    if word in punct:\n",
    "        res.append('punct')\n",
    "        return res\n",
    "    \n",
    "    ## different orthographic\n",
    "    # banary feature indicating whether the word starts with a capital letter or not, 1:yes, 0:not\n",
    "    if word.istitle():\n",
    "        res.append('start_capital')\n",
    "    # banary feature indicating whether the word is made of all capital letters or not, 1:yes, 0:not\n",
    "    if word.isupper():\n",
    "        res.append('only_capital')\n",
    "    # banary feature indicating whether the word has a digit or not, 1:yes, 0:not\n",
    "    if has_digit(word):\n",
    "        res.append('has_digit')\n",
    "    # banary feature indicating whether the word has a hyphen or not, 1:yes, 0:not\n",
    "    if '-' in word:\n",
    "        res.append('has_hyphen')\n",
    "    # banary feature indicating whether the word has a low hyphen or not, 1:yes, 0:not\n",
    "    if '_' in word:\n",
    "        res.append('has_hyphen_low')\n",
    "    # banary feature indicating whether the letters in the word are all alphanumeric or not, 1:yes, 0:not\n",
    "    if not word.isalnum():\n",
    "        res.append('not_alnum')\n",
    "    # binary feature indicating whether the length of word is more than 3\n",
    "#     if len(word) > 3:\n",
    "#         res.append('word_len_>_3')\n",
    "    \n",
    "    if '\\'' in word:\n",
    "        res.append('abbr')\n",
    "    \n",
    "    ## different morphological\n",
    "    # aient \n",
    "    \n",
    "    ## \n",
    "    # son sa ser ton \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_distribution(data, freq = 100):\n",
    "    freq_bigram_left = defaultdict(lambda: defaultdict(int))\n",
    "    freq_bigram_right = defaultdict(lambda: defaultdict(int))\n",
    "    for words, labels in data:\n",
    "        for i in range(len(words)):\n",
    "            if i > 0:\n",
    "                freq_bigram_left[words[i]][words[i-1]] += 1\n",
    "            if i < len(words)-1:\n",
    "                freq_bigram_right[words[i]][words[i+1]] += 1\n",
    "    for word, counts in freq_bigram_left.items():\n",
    "        freq_bigram_left[word] = list(sorted(counts.items(), key=lambda x : x[1], reverse=True))[:freq]\n",
    "    for word, counts in freq_bigram_right.items():\n",
    "        freq_bigram_right[word] = list(sorted(counts.items(), key=lambda x : x[1], reverse=True))[:freq]\n",
    "    return freq_bigram_left, freq_bigram_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(i, context, bigram_left, bigram_right):\n",
    "    res = feature_window(i, context)\n",
    "    res += feature_suffix(i, context)\n",
    "    res += feature_shape(i, context)\n",
    "#     for i in range(len(bigram_left)):\n",
    "#         res.append('%d-th_freq_left_'%i + bigram_left[i][0])\n",
    "#     for i in range(len(bigram_right)):\n",
    "#         res.append('%d-th_freq_right_'%i + bigram_right[i][0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data):\n",
    "    dataset = []\n",
    "    labelset = []\n",
    "    freq_bigram_left, freq_bigram_right = feature_distribution(data,freq=5)\n",
    "    for words, labels in data:\n",
    "        for i in range(len(words)):\n",
    "            dataset.append(extract_features(i, words, freq_bigram_left[words[i]], freq_bigram_right[words[i]]))\n",
    "            labelset.append(labels[i])\n",
    "    return dataset, labelset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, labels):\n",
    "        \n",
    "        self.labels = labels\n",
    "        # Each feature gets its own weight vector, with one weight for\n",
    "        # each possible label\n",
    "        self.weights = defaultdict(lambda: defaultdict(float))\n",
    "        # The accumulated values of the weight vector at the t-th\n",
    "        # iteration: sum_{i=1}^{n - 1} w_i\n",
    "        #\n",
    "        # The current value (w_t) is not yet added. The key of this\n",
    "        # dictionary is a pair (feature, label)\n",
    "        self._accum = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging.\n",
    "        self._last_update = defaultdict(int)\n",
    "        # Number of examples seen\n",
    "        self.n_updates = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return\n",
    "        the best class.'''\n",
    "        \n",
    "        # get the scores of all the labels based on the features\n",
    "        labels, labels_score = self.score(features)\n",
    "        # get the label whose socre is max\n",
    "        return labels[np.argmax(labels_score)]\n",
    "    \n",
    "    def predict_all(self, features):\n",
    "        '''\n",
    "        predict the labels based on the \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            - features, an iterable\n",
    "              WARNING: the `value' of the feature is always assumed to be 1.\n",
    "        \n",
    "        return : a list of babels predicted\n",
    "        '''\n",
    "        predicts = []\n",
    "        fp = FloatProgress(min=0, max=len(features))\n",
    "        display(fp)\n",
    "        for f in features:\n",
    "            predicts.append(self.predict(f))\n",
    "            fp.value += 1\n",
    "        return predicts\n",
    "    \n",
    "    def fit(self, train_set, train_labels):\n",
    "        '''\n",
    "        Parameters\n",
    "            - train_set: an iterable of the features of all data\n",
    "            - train_labels : an iterable of labels \n",
    "        '''\n",
    "        f = FloatProgress(min=0, max=len(train_labels))\n",
    "        display(f)\n",
    "        for features, true_label in zip(train_set, train_labels):\n",
    "            f.value += 1\n",
    "            #self.average_weights()\n",
    "            self.update(true_label, self.predict(features), features) \n",
    "    \n",
    "    def score(self, features, labels=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        - features, an iterable\n",
    "             a sequence of binary features. Each feature must be\n",
    "             hashable. WARNING: the `value' of the feature is always\n",
    "             assumed to be 1.\n",
    "        - labels, a subset of self.labels\n",
    "             if not None, the score is computed only for these labels\n",
    "        \"\"\" \n",
    "        if not labels:\n",
    "            # list of scores of the sum of weights associated with each label of all features\n",
    "            # where the index of this list is the index of list of all labels\n",
    "            labels_score = np.zeros(len(self.labels))\n",
    "            for f in features:\n",
    "                for label in self.labels:\n",
    "                    # get the weight associated by the feature and label\n",
    "                    # then add to the list of scores \n",
    "                    labels_score[self.labels.index(label)] += self.weights[f][label]\n",
    "            return self.labels, labels_score\n",
    "        else :\n",
    "            labels_score = np.zeros(len(labels))\n",
    "            for f in features:\n",
    "                for label in labels:\n",
    "                    labels_score[labels.index(label)] += self.weights[f][label]\n",
    "            return labels, labels_score\n",
    "        \n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''\n",
    "        if the true label == predicted label, then do nothing \n",
    "        else for each feature, update the associated weights of all labels \n",
    "        '''\n",
    "        def upd_feat(label, feature, v):\n",
    "            param = (label, feature)\n",
    "            self._accum[param] += (self.n_updates -\n",
    "                                   self._last_update[param]) * self.weights[feature][label]\n",
    "            self._last_update[param] = self.n_updates\n",
    "            self.weights[feature][label] += v\n",
    "            \n",
    "        self.n_updates += 1\n",
    "\n",
    "        if truth == guess:\n",
    "            return\n",
    "\n",
    "        for f in features:\n",
    "            upd_feat(truth, f, 1.0)\n",
    "            upd_feat(guess, f, -1.0)\n",
    "\n",
    "    def average_weights(self):\n",
    "        \"\"\"\n",
    "        Average weights of the perceptron\n",
    "\n",
    "        Training can no longer be resumed.\n",
    "        \"\"\"\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = defaultdict(float)\n",
    "            for label, w in weights.items():\n",
    "                param = (label, feat)\n",
    "                # Be careful not to add 1 to take into account the\n",
    "                # last weight vector (without increasing the number of\n",
    "                # iterations in the averaging)\n",
    "                total = self._accum[param] + \\\n",
    "                    (self.n_updates + 1 - self._last_update[param]) * w\n",
    "                averaged = round(total / self.n_updates, 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[label] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "    \n",
    "    def evaluate(self, test_set, test_labels):\n",
    "        import numpy as np\n",
    "        predict_labels = self.predict_all(test_set)\n",
    "        num_true = np.sum(np.array(predict_labels) == np.array(test_labels))\n",
    "        num_tatol = len(test_labels)\n",
    "        accuracy = num_true/num_tatol\n",
    "        print('true_num: %d    total_num: %d ======> accuracy : %.4f%%'%(num_true, num_tatol, accuracy*100))\n",
    "        return predict_labels\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"\n",
    "        Serialization of a perceptron\n",
    "\n",
    "        We are only serializing the weight vector as a dictionnary\n",
    "        because defaultdict with lambda can not be serialized.\n",
    "        \"\"\"\n",
    "        # should we also serialize the other attributes to allow\n",
    "        # learning to continue?\n",
    "        return {\"weights\": {k: v for k, v in self.weights.items()}}\n",
    "\n",
    "    def __setstate__(self, data):\n",
    "        \"\"\"\n",
    "        De-serialization of a perceptron\n",
    "        \"\"\"\n",
    "\n",
    "        self.weights = defaultdict(lambda: defaultdict(float), data[\"weights\"])\n",
    "        # ensure we are no longer able to continue training\n",
    "        self._accum = None\n",
    "        self._last_update = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_lables(data_set):\n",
    "    res = set()\n",
    "    for words, labels in data_set:\n",
    "        res = res | set(labels)\n",
    "    return list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = all_lables(train_set)\n",
    "train_dataset, train_labels = build_dataset(train_set)\n",
    "test_dataset, test_labels = build_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Les',\n",
       "  'none',\n",
       "  'win_i+1commotions',\n",
       "  'none',\n",
       "  'win_i+2cérébrales',\n",
       "  '-1-th_suffix_s',\n",
       "  '-2-th_suffix_es',\n",
       "  '-3-th_suffix_Les',\n",
       "  'start_capital'],\n",
       " ['commotions',\n",
       "  'win_i-1Les',\n",
       "  'win_i+1cérébrales',\n",
       "  'none',\n",
       "  'win_i+2sont',\n",
       "  '-1-th_suffix_s',\n",
       "  '-2-th_suffix_ns',\n",
       "  '-3-th_suffix_ons']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d549d0a617c04adb9053f3f9ae04117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=345009.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p.fit(train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e843a5cfc104d7aac277c997cf39855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=9742.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_num: 8132    total_num: 9742 ======> accuracy : 83.4736%\n"
     ]
    }
   ],
   "source": [
    "predicts = p.evaluate(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22784d09ce347d5af516cb13c2dc90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=10000.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_num: 9477    total_num: 10000 ======> accuracy : 94.7700%\n"
     ]
    }
   ],
   "source": [
    "predicts_train_lanels = p.evaluate(train_dataset[:10000], train_labels[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigram_left, freq_bigram_right = feature_distribution(train_set, freq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigram_left\n",
    "# for word_left in freq_bigram_left['si']:\n",
    "#     print(word_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(data, filename):\n",
    "    import pickle \n",
    "    with open(filename, 'w')as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "weights = p.__getstate__()\n",
    "save_model(weights, 'mymodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wrong = np.array(test_dataset)[np.array(predicts) != np.array(test_labels)]\n",
    "wrong_label = np.array(test_labels) [np.array(predicts) != np.array(test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in wrong:\n",
    "    print(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different orthographic\n",
    "    # banary feature indicating whether the word starts with a capital letter or not, 1:yes, 0:not\n",
    "    res.append(('start_capital',1 if word.istitle() else 0))\n",
    "    # banary feature indicating whether the word is made of all capital letters or not, 1:yes, 0:not\n",
    "    res.append(('only_capital',1 if word.isupper() else 0))\n",
    "    # banary feature indicating whether the word has a digit or not, 1:yes, 0:not\n",
    "    res.append(('has_digit', 1 if has_digit(word) else 0))\n",
    "    # banary feature indicating whether the word has a hyphen or not, 1:yes, 0:not\n",
    "    res.append(('has_hyphen', 1 if '-' in word else 0))\n",
    "    # banary feature indicating whether the word has a low hyphen or not, 1:yes, 0:not\n",
    "    res.append(('has_hyphen_low', 1 if '_' in word else 0))\n",
    "    # banary feature indicating whether the letters in the word are all alphanumeric or not, 1:yes, 0:not\n",
    "    res.append(('isalnum', 1 if word.isalnum() else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
